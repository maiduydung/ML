{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format text files into a CSV file  \n",
    "folders: target_dir/category/text_files    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import POSStopFilter, LowerCaseFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati = True\n",
    "target_dir = 'livedoor_news50'\n",
    "csv_out = 'livedoor_news50.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for wakati  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_filters = [ POSStopFilter(['助詞','助動詞']),\n",
    "                  LowerCaseFilter(),\n",
    "                ]\n",
    "tokenizer = Tokenizer()\n",
    "analyzer = Analyzer(tokenizer=tokenizer, token_filters=token_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read text files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pat = '*.txt'\n",
    "dir_pat = os.path.join(target_dir, '*')\n",
    "\n",
    "text = []\n",
    "for d in glob.glob(dir_pat):  # for-loop of directories in target_dir\n",
    "    if not os.path.isdir(d): continue  # skip if d is not a directory\n",
    "    category = os.path.basename(d)  # use directory name as the category name\n",
    "    full_pat = os.path.join(d, file_pat)  # generate full path name for text files\n",
    "    n_files = 0  # counter for number of files\n",
    "    for file in glob.glob(full_pat):  # for-loop of text files\n",
    "        n_files += 1\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines()  # if you want to omit newline character\n",
    "            body = ' '.join(lines[2:])  # 1st line: URL, 2nd line: Date, so skip these lines\n",
    "            if wakati:  # add space between words (wakati-gaki)\n",
    "                body = ' '.join([t.base_form for t in analyzer.analyze(body)])\n",
    "            text.append([category, body])\n",
    "    print(n_files, 'files read for', category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(text, columns=['category', 'text'])\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
